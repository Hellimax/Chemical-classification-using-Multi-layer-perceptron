{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step is to import the required modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data(csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./musk_csv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Little description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "      <th>f165</th>\n",
       "      <th>f166</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6598.00000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "      <td>6598.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3299.50000</td>\n",
       "      <td>58.945135</td>\n",
       "      <td>-119.128524</td>\n",
       "      <td>-73.146560</td>\n",
       "      <td>-0.628372</td>\n",
       "      <td>-103.533495</td>\n",
       "      <td>18.359806</td>\n",
       "      <td>-14.108821</td>\n",
       "      <td>-1.858290</td>\n",
       "      <td>-86.003031</td>\n",
       "      <td>...</td>\n",
       "      <td>-184.798272</td>\n",
       "      <td>-75.795696</td>\n",
       "      <td>-26.073204</td>\n",
       "      <td>64.616702</td>\n",
       "      <td>112.037739</td>\n",
       "      <td>201.760230</td>\n",
       "      <td>-47.488330</td>\n",
       "      <td>-150.259927</td>\n",
       "      <td>41.770233</td>\n",
       "      <td>0.154138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1904.82287</td>\n",
       "      <td>53.249007</td>\n",
       "      <td>90.813375</td>\n",
       "      <td>67.956235</td>\n",
       "      <td>80.444617</td>\n",
       "      <td>64.387559</td>\n",
       "      <td>80.593655</td>\n",
       "      <td>115.315673</td>\n",
       "      <td>90.372537</td>\n",
       "      <td>108.326676</td>\n",
       "      <td>...</td>\n",
       "      <td>107.819514</td>\n",
       "      <td>127.861271</td>\n",
       "      <td>69.727964</td>\n",
       "      <td>100.861935</td>\n",
       "      <td>72.835040</td>\n",
       "      <td>59.526751</td>\n",
       "      <td>55.069365</td>\n",
       "      <td>76.019023</td>\n",
       "      <td>94.116085</td>\n",
       "      <td>0.361108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-31.000000</td>\n",
       "      <td>-199.000000</td>\n",
       "      <td>-167.000000</td>\n",
       "      <td>-114.000000</td>\n",
       "      <td>-118.000000</td>\n",
       "      <td>-183.000000</td>\n",
       "      <td>-171.000000</td>\n",
       "      <td>-225.000000</td>\n",
       "      <td>-245.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-328.000000</td>\n",
       "      <td>-219.000000</td>\n",
       "      <td>-136.000000</td>\n",
       "      <td>-120.000000</td>\n",
       "      <td>-69.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>-289.000000</td>\n",
       "      <td>-428.000000</td>\n",
       "      <td>-471.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1650.25000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>-193.000000</td>\n",
       "      <td>-137.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-117.000000</td>\n",
       "      <td>-28.000000</td>\n",
       "      <td>-159.000000</td>\n",
       "      <td>-85.000000</td>\n",
       "      <td>-217.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-272.000000</td>\n",
       "      <td>-205.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-18.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>-68.000000</td>\n",
       "      <td>-179.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3299.50000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>-149.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-25.000000</td>\n",
       "      <td>-117.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-234.000000</td>\n",
       "      <td>-131.000000</td>\n",
       "      <td>-21.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-150.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4948.75000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-19.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>-116.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>-21.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>-45.000000</td>\n",
       "      <td>-120.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6598.00000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID           f1           f2           f3           f4  \\\n",
       "count  6598.00000  6598.000000  6598.000000  6598.000000  6598.000000   \n",
       "mean   3299.50000    58.945135  -119.128524   -73.146560    -0.628372   \n",
       "std    1904.82287    53.249007    90.813375    67.956235    80.444617   \n",
       "min       1.00000   -31.000000  -199.000000  -167.000000  -114.000000   \n",
       "25%    1650.25000    37.000000  -193.000000  -137.000000   -70.000000   \n",
       "50%    3299.50000    44.000000  -149.000000   -99.000000   -25.000000   \n",
       "75%    4948.75000    53.000000   -95.000000   -19.000000    42.000000   \n",
       "max    6598.00000   292.000000    95.000000    81.000000   161.000000   \n",
       "\n",
       "                f5           f6           f7           f8           f9  ...  \\\n",
       "count  6598.000000  6598.000000  6598.000000  6598.000000  6598.000000  ...   \n",
       "mean   -103.533495    18.359806   -14.108821    -1.858290   -86.003031  ...   \n",
       "std      64.387559    80.593655   115.315673    90.372537   108.326676  ...   \n",
       "min    -118.000000  -183.000000  -171.000000  -225.000000  -245.000000  ...   \n",
       "25%    -117.000000   -28.000000  -159.000000   -85.000000  -217.000000  ...   \n",
       "50%    -117.000000    33.000000    27.000000    19.000000   -40.000000  ...   \n",
       "75%    -116.000000    74.000000    57.000000    61.000000   -21.000000  ...   \n",
       "max     325.000000   200.000000   220.000000   320.000000   147.000000  ...   \n",
       "\n",
       "              f158         f159         f160         f161         f162  \\\n",
       "count  6598.000000  6598.000000  6598.000000  6598.000000  6598.000000   \n",
       "mean   -184.798272   -75.795696   -26.073204    64.616702   112.037739   \n",
       "std     107.819514   127.861271    69.727964   100.861935    72.835040   \n",
       "min    -328.000000  -219.000000  -136.000000  -120.000000   -69.000000   \n",
       "25%    -272.000000  -205.000000   -70.000000   -18.000000    71.000000   \n",
       "50%    -234.000000  -131.000000   -21.000000    61.500000   107.000000   \n",
       "75%     -80.000000    52.000000     9.000000   149.000000   129.000000   \n",
       "max      94.000000   179.000000   192.000000   411.000000   355.000000   \n",
       "\n",
       "              f163         f164         f165         f166        class  \n",
       "count  6598.000000  6598.000000  6598.000000  6598.000000  6598.000000  \n",
       "mean    201.760230   -47.488330  -150.259927    41.770233     0.154138  \n",
       "std      59.526751    55.069365    76.019023    94.116085     0.361108  \n",
       "min      73.000000  -289.000000  -428.000000  -471.000000     0.000000  \n",
       "25%     166.000000   -68.000000  -179.000000    -9.000000     0.000000  \n",
       "50%     191.000000   -60.000000  -150.000000    27.000000     0.000000  \n",
       "75%     215.000000   -45.000000  -120.000000   119.000000     0.000000  \n",
       "max     625.000000   295.000000   168.000000   367.000000     1.000000  \n",
       "\n",
       "[8 rows x 168 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.isnull().any():\n",
    "    if(i==True):\n",
    "        print(\"null value found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>conformation_name</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>...</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "      <th>f165</th>\n",
       "      <th>f166</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MUSK-211</td>\n",
       "      <td>211_1+1</td>\n",
       "      <td>46</td>\n",
       "      <td>-108</td>\n",
       "      <td>-60</td>\n",
       "      <td>-69</td>\n",
       "      <td>-117</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>-308</td>\n",
       "      <td>52</td>\n",
       "      <td>-7</td>\n",
       "      <td>39</td>\n",
       "      <td>126</td>\n",
       "      <td>156</td>\n",
       "      <td>-50</td>\n",
       "      <td>-112</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MUSK-211</td>\n",
       "      <td>211_1+10</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-6</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-59</td>\n",
       "      <td>-2</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "      <td>136</td>\n",
       "      <td>169</td>\n",
       "      <td>-61</td>\n",
       "      <td>-136</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MUSK-211</td>\n",
       "      <td>211_1+11</td>\n",
       "      <td>46</td>\n",
       "      <td>-194</td>\n",
       "      <td>-145</td>\n",
       "      <td>28</td>\n",
       "      <td>-117</td>\n",
       "      <td>73</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-134</td>\n",
       "      <td>-154</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>165</td>\n",
       "      <td>-67</td>\n",
       "      <td>-145</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MUSK-211</td>\n",
       "      <td>211_1+12</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>136</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MUSK-211</td>\n",
       "      <td>211_1+13</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>137</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID molecule_name conformation_name  f1   f2   f3  f4   f5  f6  f7  ...  \\\n",
       "0   1      MUSK-211           211_1+1  46 -108  -60 -69 -117  49  38  ...   \n",
       "1   2      MUSK-211          211_1+10  41 -188 -145  22 -117  -6  57  ...   \n",
       "2   3      MUSK-211          211_1+11  46 -194 -145  28 -117  73  57  ...   \n",
       "3   4      MUSK-211          211_1+12  41 -188 -145  22 -117  -7  57  ...   \n",
       "4   5      MUSK-211          211_1+13  41 -188 -145  22 -117  -7  57  ...   \n",
       "\n",
       "   f158  f159  f160  f161  f162  f163  f164  f165  f166  class  \n",
       "0  -308    52    -7    39   126   156   -50  -112    96      1  \n",
       "1   -59    -2    52   103   136   169   -61  -136    79      1  \n",
       "2  -134  -154    57   143   142   165   -67  -145    39      1  \n",
       "3   -60    -4    52   104   136   168   -60  -135    80      1  \n",
       "4   -60    -4    52   104   137   168   -60  -135    80      1  \n",
       "\n",
       "[5 rows x 170 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking for non integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molecule_name\n",
      "conformation_name\n"
     ]
    }
   ],
   "source": [
    "for i in data.columns:\n",
    "    if(data[i].dtype != \"int64\"):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are deleting these 2 columns because these are just names of the chemical and does not have correlation with any column of the data which means that these columns doest not help in classifying the chemicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"molecule_name\",\"conformation_name\"],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we are also deleting \"ID\" column because it also does not help in classifying the chemicals, \"ID\" is just use to uniquely identify each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"ID\",inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "      <th>f165</th>\n",
       "      <th>f166</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>-108</td>\n",
       "      <td>-60</td>\n",
       "      <td>-69</td>\n",
       "      <td>-117</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>-161</td>\n",
       "      <td>-8</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-308</td>\n",
       "      <td>52</td>\n",
       "      <td>-7</td>\n",
       "      <td>39</td>\n",
       "      <td>126</td>\n",
       "      <td>156</td>\n",
       "      <td>-50</td>\n",
       "      <td>-112</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-6</td>\n",
       "      <td>57</td>\n",
       "      <td>-171</td>\n",
       "      <td>-39</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-59</td>\n",
       "      <td>-2</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "      <td>136</td>\n",
       "      <td>169</td>\n",
       "      <td>-61</td>\n",
       "      <td>-136</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>-194</td>\n",
       "      <td>-145</td>\n",
       "      <td>28</td>\n",
       "      <td>-117</td>\n",
       "      <td>73</td>\n",
       "      <td>57</td>\n",
       "      <td>-168</td>\n",
       "      <td>-39</td>\n",
       "      <td>-22</td>\n",
       "      <td>...</td>\n",
       "      <td>-134</td>\n",
       "      <td>-154</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>165</td>\n",
       "      <td>-67</td>\n",
       "      <td>-145</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>136</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>137</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1   f2   f3  f4   f5  f6  f7   f8  f9  f10  ...  f158  f159  f160  f161  \\\n",
       "0  46 -108  -60 -69 -117  49  38 -161  -8    5  ...  -308    52    -7    39   \n",
       "1  41 -188 -145  22 -117  -6  57 -171 -39 -100  ...   -59    -2    52   103   \n",
       "2  46 -194 -145  28 -117  73  57 -168 -39  -22  ...  -134  -154    57   143   \n",
       "3  41 -188 -145  22 -117  -7  57 -170 -39  -99  ...   -60    -4    52   104   \n",
       "4  41 -188 -145  22 -117  -7  57 -170 -39  -99  ...   -60    -4    52   104   \n",
       "\n",
       "   f162  f163  f164  f165  f166  class  \n",
       "0   126   156   -50  -112    96      1  \n",
       "1   136   169   -61  -136    79      1  \n",
       "2   142   165   -67  -145    39      1  \n",
       "3   136   168   -60  -135    80      1  \n",
       "4   137   168   -60  -135    80      1  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### separating data that we have to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[\"class\"]\n",
    "data.drop(\"class\",axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f157</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "      <th>f165</th>\n",
       "      <th>f166</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>-108</td>\n",
       "      <td>-60</td>\n",
       "      <td>-69</td>\n",
       "      <td>-117</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>-161</td>\n",
       "      <td>-8</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-244</td>\n",
       "      <td>-308</td>\n",
       "      <td>52</td>\n",
       "      <td>-7</td>\n",
       "      <td>39</td>\n",
       "      <td>126</td>\n",
       "      <td>156</td>\n",
       "      <td>-50</td>\n",
       "      <td>-112</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-6</td>\n",
       "      <td>57</td>\n",
       "      <td>-171</td>\n",
       "      <td>-39</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-235</td>\n",
       "      <td>-59</td>\n",
       "      <td>-2</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "      <td>136</td>\n",
       "      <td>169</td>\n",
       "      <td>-61</td>\n",
       "      <td>-136</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>-194</td>\n",
       "      <td>-145</td>\n",
       "      <td>28</td>\n",
       "      <td>-117</td>\n",
       "      <td>73</td>\n",
       "      <td>57</td>\n",
       "      <td>-168</td>\n",
       "      <td>-39</td>\n",
       "      <td>-22</td>\n",
       "      <td>...</td>\n",
       "      <td>-238</td>\n",
       "      <td>-134</td>\n",
       "      <td>-154</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>165</td>\n",
       "      <td>-67</td>\n",
       "      <td>-145</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>-236</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>136</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>-99</td>\n",
       "      <td>...</td>\n",
       "      <td>-236</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>137</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1   f2   f3  f4   f5  f6  f7   f8  f9  f10  ...  f157  f158  f159  f160  \\\n",
       "0  46 -108  -60 -69 -117  49  38 -161  -8    5  ...  -244  -308    52    -7   \n",
       "1  41 -188 -145  22 -117  -6  57 -171 -39 -100  ...  -235   -59    -2    52   \n",
       "2  46 -194 -145  28 -117  73  57 -168 -39  -22  ...  -238  -134  -154    57   \n",
       "3  41 -188 -145  22 -117  -7  57 -170 -39  -99  ...  -236   -60    -4    52   \n",
       "4  41 -188 -145  22 -117  -7  57 -170 -39  -99  ...  -236   -60    -4    52   \n",
       "\n",
       "   f161  f162  f163  f164  f165  f166  \n",
       "0    39   126   156   -50  -112    96  \n",
       "1   103   136   169   -61  -136    79  \n",
       "2   143   142   165   -67  -145    39  \n",
       "3   104   136   168   -60  -135    80  \n",
       "4   104   137   168   -60  -135    80  \n",
       "\n",
       "[5 rows x 166 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data, here 0.2 means test data will be 20% of the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(data,target,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas = MLPClassifier(hidden_layer_sizes=(100,100,100),verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we have a model with 3 hidden layers each of 100 neurons whose activation function is \"rectified linear unit\" with solver \"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN_SAMPLES = xtrain.shape[0]\n",
    "N_EPOCHS = 25 #this is to specify how many iterations we have to make \n",
    "N_BATCH = 128 #each iteration will contain a small batch of data and this to specify how many rows will be there in each batch\n",
    "N_CLASSES = np.unique(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train = []\n",
    "scores_test = []\n",
    "loss_train = []\n",
    "loss_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 3.56537061\n",
      "Iteration 3, loss = inf\n",
      "Iteration 4, loss = inf\n",
      "Iteration 5, loss = 4.39896099\n",
      "Iteration 6, loss = 4.09948894\n",
      "Iteration 7, loss = 2.52932228\n",
      "Iteration 8, loss = 1.29683160\n",
      "Iteration 9, loss = 1.87235303\n",
      "Iteration 10, loss = 2.19023972\n",
      "Iteration 11, loss = 1.60228236\n",
      "Iteration 12, loss = 2.29298014\n",
      "Iteration 13, loss = 1.52011964\n",
      "Iteration 14, loss = 1.08481285\n",
      "Iteration 15, loss = 2.17348983\n",
      "Iteration 16, loss = 1.08342827\n",
      "Iteration 17, loss = 1.02318272\n",
      "Iteration 18, loss = 1.26571034\n",
      "Iteration 19, loss = 0.80665414\n",
      "Iteration 20, loss = 0.89805465\n",
      "Iteration 21, loss = 0.65720840\n",
      "Iteration 22, loss = 0.99579682\n",
      "Iteration 23, loss = 1.46502077\n",
      "Iteration 24, loss = 1.45827668\n",
      "Iteration 25, loss = 0.99199248\n",
      "Iteration 26, loss = 0.46158814\n",
      "Iteration 27, loss = 0.90651194\n",
      "Iteration 28, loss = 1.03902251\n",
      "Iteration 29, loss = 0.92242613\n",
      "Iteration 30, loss = 0.11867532\n",
      "Iteration 31, loss = 0.56788081\n",
      "Iteration 32, loss = 1.03445257\n",
      "Iteration 33, loss = 0.47055276\n",
      "Iteration 34, loss = 0.37832276\n",
      "Iteration 35, loss = 0.59928322\n",
      "Iteration 36, loss = 0.30098314\n",
      "Iteration 37, loss = 0.79238761\n",
      "Iteration 38, loss = 0.23061306\n",
      "Iteration 39, loss = 1.04121733\n",
      "Iteration 40, loss = 0.44859334\n",
      "Iteration 41, loss = 0.59000842\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 2.22144889\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  1\n",
      "Iteration 43, loss = 0.68510217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.72477735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.34562045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.35155069\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.16750165\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.60027491\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 0.33767469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 0.56131380\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.25538111\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.45377657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.26335229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 0.22602098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 55, loss = 0.50700453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 0.24245472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.36401490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.17481916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 59, loss = 0.28628810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 60, loss = 0.10031938\n",
      "Iteration 61, loss = 0.09332857\n",
      "Iteration 62, loss = 0.77624072\n",
      "Iteration 63, loss = 0.16753195\n",
      "Iteration 64, loss = 0.20673816\n",
      "Iteration 65, loss = 0.18141796\n",
      "Iteration 66, loss = 0.51267577\n",
      "Iteration 67, loss = 0.38032304\n",
      "Iteration 68, loss = 0.16984741\n",
      "Iteration 69, loss = 0.43686709\n",
      "Iteration 70, loss = 0.20121614\n",
      "Iteration 71, loss = 0.47740268\n",
      "Iteration 72, loss = 0.35601586\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 73, loss = 0.18790334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 74, loss = 0.48982689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 75, loss = 0.47387891\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 76, loss = 0.31945298\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 77, loss = 0.11375125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 78, loss = 0.26574246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 79, loss = 0.26438684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 80, loss = 0.18581226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 81, loss = 0.09647526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 82, loss = 0.26150726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 83, loss = 0.45982547\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 84, loss = 0.27073953\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  2\n",
      "Iteration 85, loss = 0.15151105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 86, loss = 0.25048556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 87, loss = 0.11213148\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 88, loss = 0.19695711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 89, loss = 0.26526421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 90, loss = 0.15448804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 91, loss = 0.14894280\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 92, loss = 0.15134914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 93, loss = 0.22788008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 94, loss = 0.17941617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 95, loss = 0.10769369\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 96, loss = 0.06494221\n",
      "Iteration 97, loss = 0.08183119\n",
      "Iteration 98, loss = 0.27832330\n",
      "Iteration 99, loss = 0.24068802\n",
      "Iteration 100, loss = 0.12230532\n",
      "Iteration 101, loss = 0.14317430\n",
      "Iteration 102, loss = 0.17076371\n",
      "Iteration 103, loss = 0.26262748\n",
      "Iteration 104, loss = 0.12092010\n",
      "Iteration 105, loss = 0.07402482\n",
      "Iteration 106, loss = 0.10694820\n",
      "Iteration 107, loss = 0.11082086\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 108, loss = 0.05295254\n",
      "Iteration 109, loss = 0.09011364\n",
      "Iteration 110, loss = 0.14069585\n",
      "Iteration 111, loss = 0.15180130\n",
      "Iteration 112, loss = 0.09663518\n",
      "Iteration 113, loss = 0.20763430\n",
      "Iteration 114, loss = 0.10739544\n",
      "Iteration 115, loss = 0.04933665\n",
      "Iteration 116, loss = 0.20530677\n",
      "Iteration 117, loss = 0.10096416\n",
      "Iteration 118, loss = 0.17494125\n",
      "Iteration 119, loss = 0.21518040\n",
      "Iteration 120, loss = 0.04348627\n",
      "Iteration 121, loss = 0.10616398\n",
      "Iteration 122, loss = 0.01117986\n",
      "Iteration 123, loss = 0.03695882\n",
      "Iteration 124, loss = 0.41289861\n",
      "Iteration 125, loss = 0.16605562\n",
      "Iteration 126, loss = 0.01163518\n",
      "epoch:  3\n",
      "Iteration 127, loss = 0.09650825\n",
      "Iteration 128, loss = 0.14804752\n",
      "Iteration 129, loss = 0.18924140\n",
      "Iteration 130, loss = 0.08367419\n",
      "Iteration 131, loss = 0.02107236\n",
      "Iteration 132, loss = 0.08730123\n",
      "Iteration 133, loss = 0.17708545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 134, loss = 0.04055012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 135, loss = 0.13164931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 136, loss = 0.08789358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 137, loss = 0.22286897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 138, loss = 0.04300671\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 139, loss = 0.10143457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 140, loss = 0.23167704\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 141, loss = 0.12665802\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 142, loss = 0.06301261\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 143, loss = 0.15308154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 144, loss = 0.09112173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 145, loss = 0.18616831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 146, loss = 0.09873405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 147, loss = 0.01797295\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 148, loss = 0.08416404\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 149, loss = 0.23207756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 150, loss = 0.09495676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 151, loss = 0.28820264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 152, loss = 0.12118806\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 153, loss = 0.04158069\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 154, loss = 0.02980224\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 155, loss = 0.19489918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 156, loss = 0.10664370\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 157, loss = 0.16812386\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 158, loss = 0.16433965\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 159, loss = 0.08954059\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 160, loss = 0.08403935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 161, loss = 0.11387844\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 162, loss = 0.22633982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 163, loss = 0.20151675\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 164, loss = 0.16329355\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 165, loss = 0.19142824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 166, loss = 0.12831684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 167, loss = 0.37238098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 168, loss = 0.00835703\n",
      "epoch:  4\n",
      "Iteration 169, loss = 0.15046557\n",
      "Iteration 170, loss = 0.11172126\n",
      "Iteration 171, loss = 0.03640701\n",
      "Iteration 172, loss = 0.03692514\n",
      "Iteration 173, loss = 0.08639987\n",
      "Iteration 174, loss = 0.07476270\n",
      "Iteration 175, loss = 0.18194754\n",
      "Iteration 176, loss = 0.10146653\n",
      "Iteration 177, loss = 0.18193274\n",
      "Iteration 178, loss = 0.22767770\n",
      "Iteration 179, loss = 0.01862340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 180, loss = 0.07674193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 181, loss = 0.00742493\n",
      "Iteration 182, loss = 0.11721386\n",
      "Iteration 183, loss = 0.09469904\n",
      "Iteration 184, loss = 0.12760253\n",
      "Iteration 185, loss = 0.22462892\n",
      "Iteration 186, loss = 0.08066319\n",
      "Iteration 187, loss = 0.13656515\n",
      "Iteration 188, loss = 0.21169700\n",
      "Iteration 189, loss = 0.22195195\n",
      "Iteration 190, loss = 0.02669209\n",
      "Iteration 191, loss = 0.06597956\n",
      "Iteration 192, loss = 0.12182841\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 193, loss = 0.01939176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 194, loss = 0.20986465\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 195, loss = 0.12680307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 196, loss = 0.03123166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 197, loss = 0.14674206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 198, loss = 0.11919186\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 199, loss = 0.05654909\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 200, loss = 0.04839620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 201, loss = 0.09261602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 202, loss = 0.04772496\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 203, loss = 0.08838676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 204, loss = 0.21448592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 205, loss = 0.13459440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 206, loss = 0.06823122\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 207, loss = 0.05820140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 208, loss = 0.02975159\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 209, loss = 0.12479073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 210, loss = 0.40392934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  5\n",
      "Iteration 211, loss = 0.02179424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 212, loss = 0.03089722\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 213, loss = 0.20551115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 214, loss = 0.38171790\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 215, loss = 0.15044290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 216, loss = 0.01536989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 217, loss = 0.13219992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 218, loss = 0.18605946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 219, loss = 0.12386758\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 220, loss = 0.05540632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 221, loss = 0.14637299\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 222, loss = 0.22896618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 223, loss = 0.20006387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 224, loss = 0.05394140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 225, loss = 0.02298985\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 226, loss = 0.11641794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 227, loss = 0.06612792\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 228, loss = 0.17728564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 229, loss = 0.24361514\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 230, loss = 0.05919199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 231, loss = 0.28684340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 232, loss = 0.10755322\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 233, loss = 0.13713540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 234, loss = 0.02217223\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 235, loss = 0.11041818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 236, loss = 0.08440334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 237, loss = 0.16212483\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 238, loss = 0.09023996\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 239, loss = 0.11087929\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 240, loss = 0.14505354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 241, loss = 0.16023550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 242, loss = 0.07347313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 243, loss = 0.32439102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 244, loss = 0.14695270\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 245, loss = 0.02622107\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 246, loss = 0.07126550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 247, loss = 0.06058059\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 248, loss = 0.15087921\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 249, loss = 0.18046375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 250, loss = 0.15867646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 251, loss = 0.18858288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 252, loss = 0.00087183\n",
      "epoch:  6\n",
      "Iteration 253, loss = 0.04189422\n",
      "Iteration 254, loss = 0.06405866\n",
      "Iteration 255, loss = 0.16761011\n",
      "Iteration 256, loss = 0.02319851\n",
      "Iteration 257, loss = 0.06357034\n",
      "Iteration 258, loss = 0.02700600\n",
      "Iteration 259, loss = 0.04366356\n",
      "Iteration 260, loss = 0.07953907\n",
      "Iteration 261, loss = 0.05920720\n",
      "Iteration 262, loss = 0.01472271\n",
      "Iteration 263, loss = 0.06368289\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 264, loss = 0.04949748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 265, loss = 0.04132236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 266, loss = 0.11624243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 267, loss = 0.09221818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 268, loss = 0.03812888\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 269, loss = 0.07314976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 270, loss = 0.04592800\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 271, loss = 0.07692525\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 272, loss = 0.04575078\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 273, loss = 0.02485828\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 274, loss = 0.03444549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 275, loss = 0.05122354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 276, loss = 0.07120504\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 277, loss = 0.06301905\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 278, loss = 0.05680061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 279, loss = 0.07575896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 280, loss = 0.12437276\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 281, loss = 0.01801227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 282, loss = 0.06617906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 283, loss = 0.01314982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 284, loss = 0.03776673\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 285, loss = 0.06545006\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 286, loss = 0.01248185\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 287, loss = 0.04398821\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 288, loss = 0.04891478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 289, loss = 0.03617184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 290, loss = 0.02357063\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 291, loss = 0.04922757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 292, loss = 0.01722817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 293, loss = 0.05105023\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 294, loss = 0.00475669\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  7\n",
      "Iteration 295, loss = 0.01955184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 296, loss = 0.00874732\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 297, loss = 0.01484503\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 298, loss = 0.02317218\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 299, loss = 0.00743387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 300, loss = 0.01613012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 301, loss = 0.00309704\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 302, loss = 0.00622816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 303, loss = 0.00723077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 304, loss = 0.01385907\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 305, loss = 0.03819429\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 306, loss = 0.04641420\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 307, loss = 0.00958272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 308, loss = 0.01365985\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 309, loss = 0.01731432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 310, loss = 0.03567493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 311, loss = 0.02592039\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 312, loss = 0.00391367\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 313, loss = 0.00856306\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 314, loss = 0.01517300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 315, loss = 0.00135054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 316, loss = 0.02760667\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 317, loss = 0.01809622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 318, loss = 0.01834144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 319, loss = 0.05066715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 320, loss = 0.00858215\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 321, loss = 0.00383913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 322, loss = 0.06895390\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 323, loss = 0.00362873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 324, loss = 0.05408427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 325, loss = 0.00097201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 326, loss = 0.04197688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 327, loss = 0.08757980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 328, loss = 0.01396490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 329, loss = 0.03421957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 330, loss = 0.01712570\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 331, loss = 0.00271622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 332, loss = 0.00707826\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 333, loss = 0.06046338\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 334, loss = 0.02256625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 335, loss = 0.04217840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 336, loss = 0.00069989\n",
      "epoch:  8\n",
      "Iteration 337, loss = 0.00963689\n",
      "Iteration 338, loss = 0.07308096\n",
      "Iteration 339, loss = 0.07521830\n",
      "Iteration 340, loss = 0.00502218\n",
      "Iteration 341, loss = 0.01208240\n",
      "Iteration 342, loss = 0.05790629\n",
      "Iteration 343, loss = 0.03024176\n",
      "Iteration 344, loss = 0.04976825\n",
      "Iteration 345, loss = 0.02789197\n",
      "Iteration 346, loss = 0.04960469\n",
      "Iteration 347, loss = 0.01348937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 348, loss = 0.02680423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 349, loss = 0.05271795\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 350, loss = 0.16128659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 351, loss = 0.06942383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 352, loss = 0.02709943\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 353, loss = 0.00354840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 354, loss = 0.06907169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 355, loss = 0.23809142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 356, loss = 0.02885029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 357, loss = 0.05243058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 358, loss = 0.00526181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 359, loss = 0.00436037\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 360, loss = 0.09425860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 361, loss = 0.04814872\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 362, loss = 0.01995285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 363, loss = 0.00726875\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 364, loss = 0.02954043\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 365, loss = 0.01369574\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 366, loss = 0.06346745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 367, loss = 0.03217107\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 368, loss = 0.10230355\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 369, loss = 0.02779019\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 370, loss = 0.03813520\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 371, loss = 0.02090196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 372, loss = 0.00723692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 373, loss = 0.13730053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 374, loss = 0.07496584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 375, loss = 0.01883905\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 376, loss = 0.08144379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 377, loss = 0.03065880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 378, loss = 0.09802600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  9\n",
      "Iteration 379, loss = 0.00241493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 380, loss = 0.00064072\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 381, loss = 0.04629870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 382, loss = 0.04652761\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 383, loss = 0.07705932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 384, loss = 0.07266647\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 385, loss = 0.04889066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 386, loss = 0.10753576\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 387, loss = 0.06225520\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 388, loss = 0.01125898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 389, loss = 0.00246817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 390, loss = 0.00062024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 391, loss = 0.01318011\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 392, loss = 0.14398456\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 393, loss = 0.03841132\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 394, loss = 0.03162566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 395, loss = 0.00158099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 396, loss = 0.08909660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 397, loss = 0.05665533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 398, loss = 0.10453308\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 399, loss = 0.07182308\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 400, loss = 0.00136716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 401, loss = 0.06497084\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 402, loss = 0.02908624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 403, loss = 0.00769042\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 404, loss = 0.00622594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 405, loss = 0.02341745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 406, loss = 0.04471676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 407, loss = 0.09371633\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 408, loss = 0.02124230\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 409, loss = 0.07575816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 410, loss = 0.04640116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 411, loss = 0.00929594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 412, loss = 0.00677255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 413, loss = 0.04098294\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 414, loss = 0.07318969\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 415, loss = 0.20544077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 416, loss = 0.03693513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 417, loss = 0.00431914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 418, loss = 0.02287860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 419, loss = 0.07804461\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 420, loss = 0.18619105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  10\n",
      "Iteration 421, loss = 0.01014713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 422, loss = 0.00452928\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 423, loss = 0.03854855\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 424, loss = 0.28279373\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 425, loss = 0.03246020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 426, loss = 0.04079440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 427, loss = 0.03370449\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 428, loss = 0.01687998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 429, loss = 0.21274787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 430, loss = 0.03057794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 431, loss = 0.11285352\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 432, loss = 0.01928348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 433, loss = 0.06012745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 434, loss = 0.02288272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 435, loss = 0.05846286\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 436, loss = 0.26818511\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 437, loss = 0.27007411\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 438, loss = 0.21364290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 439, loss = 0.16972799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 440, loss = 0.00709854\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 441, loss = 0.07782287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 442, loss = 0.26647977\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 443, loss = 0.23746632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 444, loss = 0.03476272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 445, loss = 0.00176876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 446, loss = 0.00540861\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 447, loss = 0.06571258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 448, loss = 0.12876348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 449, loss = 0.10720971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 450, loss = 0.12480232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 451, loss = 0.00074950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 452, loss = 0.10754591\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 453, loss = 0.01565195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 454, loss = 0.02674321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 455, loss = 0.26492177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 456, loss = 0.00881010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 457, loss = 0.04964690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 458, loss = 0.00970222\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 459, loss = 0.14534555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 460, loss = 0.03593111\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 461, loss = 0.05297809\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 462, loss = 0.36383893\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  11\n",
      "Iteration 463, loss = 0.03659818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 464, loss = 0.00873549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 465, loss = 0.05450568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 466, loss = 0.01961648\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 467, loss = 0.16197344\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 468, loss = 0.03120556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 469, loss = 0.04910663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 470, loss = 0.00220642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 471, loss = 0.00937191\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 472, loss = 0.12466197\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 473, loss = 0.00669767\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 474, loss = 0.00176657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 475, loss = 0.08541590\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 476, loss = 0.06913260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 477, loss = 0.01630592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 478, loss = 0.01546472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 479, loss = 0.04510679\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 480, loss = 0.10324533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 481, loss = 0.19946058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 482, loss = 0.02491215\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 483, loss = 0.00719447\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 484, loss = 0.11024632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 485, loss = 0.09226535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 486, loss = 0.14855529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 487, loss = 0.04968432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 488, loss = 0.04939301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 489, loss = 0.01835650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 490, loss = 0.02242590\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 491, loss = 0.06055110\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 492, loss = 0.12500809\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 493, loss = 0.07587148\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 494, loss = 0.05377461\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 495, loss = 0.01176428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 496, loss = 0.08654988\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 497, loss = 0.04891220\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 498, loss = 0.11814393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 499, loss = 0.19026351\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 500, loss = 0.05739829\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 501, loss = 0.00284529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 502, loss = 0.01963348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 503, loss = 0.00392831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 504, loss = 0.00128836\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  12\n",
      "Iteration 505, loss = 0.04966094\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 506, loss = 0.04071426\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 507, loss = 0.00653709\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 508, loss = 0.00827317\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 509, loss = 0.00090027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 510, loss = 0.05799235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 511, loss = 0.09288729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 512, loss = 0.03280363\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 513, loss = 0.00151677\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 514, loss = 0.03079739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 515, loss = 0.03204566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 516, loss = 0.06651223\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 517, loss = 0.02457386\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 518, loss = 0.01198313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 519, loss = 0.01730618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 520, loss = 0.02541530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 521, loss = 0.01094535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 522, loss = 0.08321325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 523, loss = 0.00802189\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 524, loss = 0.00789399\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 525, loss = 0.03709034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 526, loss = 0.04618208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 527, loss = 0.00944354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 528, loss = 0.00139729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 529, loss = 0.05956906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 530, loss = 0.11477545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 531, loss = 0.12372168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 532, loss = 0.02449502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 533, loss = 0.03303335\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 534, loss = 0.01307968\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 535, loss = 0.00609354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 536, loss = 0.07256556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 537, loss = 0.04399260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 538, loss = 0.04092251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 539, loss = 0.03539002\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 540, loss = 0.00671271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 541, loss = 0.04848298\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 542, loss = 0.01846296\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 543, loss = 0.03462859\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 544, loss = 0.03413485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 545, loss = 0.01753127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 546, loss = 0.01750061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  13\n",
      "Iteration 547, loss = 0.02116770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 548, loss = 0.14799655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 549, loss = 0.02949210\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 550, loss = 0.02831805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 551, loss = 0.06001258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 552, loss = 0.01218084\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 553, loss = 0.04102029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 554, loss = 0.03797229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 555, loss = 0.00846542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 556, loss = 0.03840833\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 557, loss = 0.13602771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 558, loss = 0.08933074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 559, loss = 0.07548798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 560, loss = 0.01337617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 561, loss = 0.02485819\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 562, loss = 0.04680243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 563, loss = 0.02390123\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 564, loss = 0.01771687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 565, loss = 0.03250249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 566, loss = 0.00188396\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 567, loss = 0.08021194\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 568, loss = 0.00151464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 569, loss = 0.06246450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 570, loss = 0.00509942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 571, loss = 0.03489721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 572, loss = 0.04168003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 573, loss = 0.00868639\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 574, loss = 0.03759232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 575, loss = 0.02341334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 576, loss = 0.05904229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 577, loss = 0.00433327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 578, loss = 0.00947884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 579, loss = 0.00067947\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 580, loss = 0.00359801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 581, loss = 0.00857329\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 582, loss = 0.01800002\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 583, loss = 0.06510689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 584, loss = 0.04996475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 585, loss = 0.02246306\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 586, loss = 0.00969806\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 587, loss = 0.00805724\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 588, loss = 0.00106182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  14\n",
      "Iteration 589, loss = 0.09493461\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 590, loss = 0.03556442\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 591, loss = 0.01084281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 592, loss = 0.00059281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 593, loss = 0.00685584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 594, loss = 0.00460198\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 595, loss = 0.02807868\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 596, loss = 0.00315688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 597, loss = 0.06440475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 598, loss = 0.00340336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 599, loss = 0.03442606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 600, loss = 0.05957151\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 601, loss = 0.02938075\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 602, loss = 0.04682338\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 603, loss = 0.00407246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 604, loss = 0.00896926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 605, loss = 0.07039339\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 606, loss = 0.02116634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 607, loss = 0.00833259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 608, loss = 0.00326010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 609, loss = 0.00918034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 610, loss = 0.01015828\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 611, loss = 0.04204537\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 612, loss = 0.09423585\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 613, loss = 0.05541064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 614, loss = 0.00459205\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 615, loss = 0.01483054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 616, loss = 0.03357445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 617, loss = 0.06919156\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 618, loss = 0.03665030\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 619, loss = 0.01069044\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 620, loss = 0.04772249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 621, loss = 0.01088404\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 622, loss = 0.03341842\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 623, loss = 0.09771188\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 624, loss = 0.01136244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 625, loss = 0.00945415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 626, loss = 0.04940222\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 627, loss = 0.06013427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 628, loss = 0.03497411\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 629, loss = 0.01336875\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 630, loss = 0.00052129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  15\n",
      "Iteration 631, loss = 0.00167442\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 632, loss = 0.12861418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 633, loss = 0.00206027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 634, loss = 0.00119161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 635, loss = 0.01495830\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 636, loss = 0.00242569\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 637, loss = 0.02218598\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 638, loss = 0.05085338\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 639, loss = 0.00264991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 640, loss = 0.02316079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 641, loss = 0.00188435\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 642, loss = 0.00884583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 643, loss = 0.00577017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 644, loss = 0.02091492\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 645, loss = 0.00057204\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 646, loss = 0.01067214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 647, loss = 0.03313820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 648, loss = 0.03143034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 649, loss = 0.00838595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 650, loss = 0.02744287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 651, loss = 0.00985874\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 652, loss = 0.00140577\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 653, loss = 0.01041291\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 654, loss = 0.00802208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 655, loss = 0.00051529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 656, loss = 0.00026125\n",
      "Iteration 657, loss = 0.00626145\n",
      "Iteration 658, loss = 0.00225118\n",
      "Iteration 659, loss = 0.01184073\n",
      "Iteration 660, loss = 0.08517441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 661, loss = 0.00177240\n",
      "Iteration 662, loss = 0.01011887\n",
      "Iteration 663, loss = 0.00033236\n",
      "Iteration 664, loss = 0.01463480\n",
      "Iteration 665, loss = 0.00098363\n",
      "Iteration 666, loss = 0.10493984\n",
      "Iteration 667, loss = 0.02703700\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 668, loss = 0.00155015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 669, loss = 0.00176334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 670, loss = 0.06129764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 671, loss = 0.04418235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 672, loss = 0.03719071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  16\n",
      "Iteration 673, loss = 0.01378441\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 674, loss = 0.02887815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 675, loss = 0.00164186\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 676, loss = 0.03309703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 677, loss = 0.12038626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 678, loss = 0.01467483\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 679, loss = 0.00236708\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 680, loss = 0.01039083\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 681, loss = 0.01039918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 682, loss = 0.00633713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 683, loss = 0.04360919\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 684, loss = 0.04157002\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 685, loss = 0.00681227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 686, loss = 0.00537255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 687, loss = 0.00356719\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 688, loss = 0.05323521\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 689, loss = 0.00686656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 690, loss = 0.04673196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 691, loss = 0.00161099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 692, loss = 0.01532983\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 693, loss = 0.00372897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 694, loss = 0.03369250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 695, loss = 0.02762878\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 696, loss = 0.00259795\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 697, loss = 0.00825728\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 698, loss = 0.00252510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 699, loss = 0.02169920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 700, loss = 0.01495750\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 701, loss = 0.01449939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 702, loss = 0.00509022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 703, loss = 0.01683996\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 704, loss = 0.00055454\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 705, loss = 0.03116691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 706, loss = 0.00365738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 707, loss = 0.03636032\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 708, loss = 0.05378949\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 709, loss = 0.01816604\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 710, loss = 0.00471141\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 711, loss = 0.00242146\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 712, loss = 0.01043339\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 713, loss = 0.00348488\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 714, loss = 0.00052892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  17\n",
      "Iteration 715, loss = 0.00182262\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 716, loss = 0.00243275\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 717, loss = 0.00605990\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 718, loss = 0.00087436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 719, loss = 0.00222136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 720, loss = 0.01692213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 721, loss = 0.00112303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 722, loss = 0.00031376\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 723, loss = 0.01510080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 724, loss = 0.02349923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 725, loss = 0.00021105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 726, loss = 0.01106436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 727, loss = 0.00271305\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 728, loss = 0.00024300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 729, loss = 0.00065202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 730, loss = 0.00121150\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 731, loss = 0.00235235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 732, loss = 0.00429036\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 733, loss = 0.01288562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 734, loss = 0.00200340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 735, loss = 0.00082723\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 736, loss = 0.03577611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 737, loss = 0.00339460\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 738, loss = 0.00473732\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 739, loss = 0.00345631\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 740, loss = 0.00701827\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 741, loss = 0.00957910\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 742, loss = 0.00048317\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 743, loss = 0.00184661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 744, loss = 0.00367146\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 745, loss = 0.00704767\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 746, loss = 0.02883699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 747, loss = 0.02491143\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 748, loss = 0.00055373\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 749, loss = 0.00266521\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 750, loss = 0.00832769\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 751, loss = 0.00447237\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 752, loss = 0.01812815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 753, loss = 0.00178358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 754, loss = 0.02559966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 755, loss = 0.00730991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 756, loss = 0.00079064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  18\n",
      "Iteration 757, loss = 0.00033410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 758, loss = 0.00074686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 759, loss = 0.00996465\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 760, loss = 0.00015275\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 761, loss = 0.01330312\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 762, loss = 0.03776655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 763, loss = 0.00164791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 764, loss = 0.00156302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 765, loss = 0.00427660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 766, loss = 0.00939863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 767, loss = 0.00107484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 768, loss = 0.00263053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 769, loss = 0.01528896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 770, loss = 0.00358957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 771, loss = 0.00289302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 772, loss = 0.00126698\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 773, loss = 0.00474753\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 774, loss = 0.03576485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 775, loss = 0.01643296\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 776, loss = 0.00173643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 777, loss = 0.00176615\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 778, loss = 0.00046975\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 779, loss = 0.00398762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 780, loss = 0.01826787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 781, loss = 0.00155736\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 782, loss = 0.00227668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 783, loss = 0.01359969\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 784, loss = 0.01734942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 785, loss = 0.01154395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 786, loss = 0.00232903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 787, loss = 0.02426842\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 788, loss = 0.00694136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 789, loss = 0.00172442\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 790, loss = 0.00719403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 791, loss = 0.00265977\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 792, loss = 0.03112293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 793, loss = 0.00856488\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 794, loss = 0.00036539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 795, loss = 0.00289144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 796, loss = 0.00041935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 797, loss = 0.00327119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 798, loss = 0.00130995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  19\n",
      "Iteration 799, loss = 0.00111290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 800, loss = 0.00032148\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 801, loss = 0.00346531\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 802, loss = 0.00036929\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 803, loss = 0.00162496\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 804, loss = 0.00232737\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 805, loss = 0.00016612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 806, loss = 0.00072129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 807, loss = 0.02009176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 808, loss = 0.00971769\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 809, loss = 0.00301053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 810, loss = 0.00121756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 811, loss = 0.00147840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 812, loss = 0.00081480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 813, loss = 0.00035006\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 814, loss = 0.02001128\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 815, loss = 0.00347050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 816, loss = 0.01037038\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 817, loss = 0.00146797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 818, loss = 0.00091363\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 819, loss = 0.00076302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 820, loss = 0.00084828\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 821, loss = 0.00041698\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 822, loss = 0.00187039\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 823, loss = 0.00066183\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 824, loss = 0.00247142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 825, loss = 0.00024819\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 826, loss = 0.00900258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 827, loss = 0.00027280\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 828, loss = 0.00098281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 829, loss = 0.00018560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 830, loss = 0.00014520\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 831, loss = 0.00686141\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 832, loss = 0.00429986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 833, loss = 0.00296227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 834, loss = 0.00185142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 835, loss = 0.00287540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 836, loss = 0.00117504\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 837, loss = 0.00077514\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 838, loss = 0.00068391\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 839, loss = 0.00057814\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 840, loss = 0.00061207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  20\n",
      "Iteration 841, loss = 0.00136017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 842, loss = 0.00041136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 843, loss = 0.01085457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 844, loss = 0.00044097\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 845, loss = 0.00152496\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 846, loss = 0.00077481\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 847, loss = 0.00109899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 848, loss = 0.00213303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 849, loss = 0.00095065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 850, loss = 0.00058641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 851, loss = 0.00147089\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 852, loss = 0.00031416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 853, loss = 0.00063581\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 854, loss = 0.00062518\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 855, loss = 0.00031941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 856, loss = 0.00134218\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 857, loss = 0.00146436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 858, loss = 0.00143760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 859, loss = 0.00022262\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 860, loss = 0.00033801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 861, loss = 0.00028007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 862, loss = 0.00189676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 863, loss = 0.00040078\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 864, loss = 0.00036106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 865, loss = 0.00015583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 866, loss = 0.00043975\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 867, loss = 0.00048453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 868, loss = 0.00374364\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 869, loss = 0.00068602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 870, loss = 0.00211241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 871, loss = 0.00064450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 872, loss = 0.00015854\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 873, loss = 0.00031325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 874, loss = 0.03187771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 875, loss = 0.00152090\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 876, loss = 0.00020768\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 877, loss = 0.00056235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 878, loss = 0.00015735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 879, loss = 0.00214427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 880, loss = 0.00038130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 881, loss = 0.00669966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 882, loss = 0.00062952\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  21\n",
      "Iteration 883, loss = 0.00342427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 884, loss = 0.00035984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 885, loss = 0.01938231\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 886, loss = 0.00034616\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 887, loss = 0.00016429\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 888, loss = 0.00266232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 889, loss = 0.00488940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 890, loss = 0.00064833\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 891, loss = 0.00164769\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 892, loss = 0.00032696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 893, loss = 0.00128430\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 894, loss = 0.00086293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 895, loss = 0.00056262\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 896, loss = 0.00020773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 897, loss = 0.00046639\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 898, loss = 0.00411510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 899, loss = 0.00125055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 900, loss = 0.00365508\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 901, loss = 0.00180378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 902, loss = 0.00017591\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 903, loss = 0.00042355\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 904, loss = 0.00066884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 905, loss = 0.00022308\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 906, loss = 0.00086449\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 907, loss = 0.00147922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 908, loss = 0.00074192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 909, loss = 0.00023865\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 910, loss = 0.00033324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 911, loss = 0.00034754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 912, loss = 0.00242022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 913, loss = 0.00108374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 914, loss = 0.00027243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 915, loss = 0.00015065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 916, loss = 0.00961417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 917, loss = 0.00046851\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 918, loss = 0.00788075\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 919, loss = 0.00099656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 920, loss = 0.00127898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 921, loss = 0.00382653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 922, loss = 0.00162008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 923, loss = 0.00029635\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 924, loss = 0.00052293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  22\n",
      "Iteration 925, loss = 0.00610163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 926, loss = 0.00082499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 927, loss = 0.00015291\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 928, loss = 0.00079237\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 929, loss = 0.00041634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 930, loss = 0.00064734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 931, loss = 0.00070984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 932, loss = 0.00050534\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 933, loss = 0.00044381\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 934, loss = 0.00020110\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 935, loss = 0.00021545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 936, loss = 0.00025121\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 937, loss = 0.00045635\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 938, loss = 0.00117850\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 939, loss = 0.00019153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 940, loss = 0.00043280\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 941, loss = 0.00046503\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 942, loss = 0.00014296\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 943, loss = 0.00020660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 944, loss = 0.00032122\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 945, loss = 0.00059863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 946, loss = 0.00031610\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 947, loss = 0.00082300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 948, loss = 0.00024727\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 949, loss = 0.00082589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 950, loss = 0.00023216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 951, loss = 0.00016095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 952, loss = 0.00017127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 953, loss = 0.00085466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 954, loss = 0.00030668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 955, loss = 0.00257140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 956, loss = 0.00085253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 957, loss = 0.00034587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 958, loss = 0.00034916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 959, loss = 0.00019413\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 960, loss = 0.00014135\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 961, loss = 0.00050908\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 962, loss = 0.00023553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 963, loss = 0.00087606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 964, loss = 0.00027244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 965, loss = 0.00513767\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 966, loss = 0.00056015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  23\n",
      "Iteration 967, loss = 0.00016866\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 968, loss = 0.00014157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 969, loss = 0.00018102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 970, loss = 0.00017349\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 971, loss = 0.00129760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 972, loss = 0.00181756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 973, loss = 0.00037049\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 974, loss = 0.00061853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 975, loss = 0.00080740\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 976, loss = 0.00042219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 977, loss = 0.00064167\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 978, loss = 0.00024830\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 979, loss = 0.00021096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 980, loss = 0.00044481\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 981, loss = 0.00030407\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 982, loss = 0.00192702\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 983, loss = 0.00042828\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 984, loss = 0.00033410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 985, loss = 0.00031806\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 986, loss = 0.00047707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 987, loss = 0.00066845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 988, loss = 0.00062626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 989, loss = 0.00043156\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 990, loss = 0.00018763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 991, loss = 0.00021889\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 992, loss = 0.00016264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 993, loss = 0.00034339\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 994, loss = 0.00037275\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 995, loss = 0.00026551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 996, loss = 0.00056060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 997, loss = 0.00046463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 998, loss = 0.00020277\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 999, loss = 0.00029096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1000, loss = 0.00025159\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1001, loss = 0.00068565\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1002, loss = 0.00050577\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1003, loss = 0.00057009\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1004, loss = 0.00028324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1005, loss = 0.00014080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1006, loss = 0.00061994\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1007, loss = 0.00071676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1008, loss = 0.00094501\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "epoch:  24\n",
      "Iteration 1009, loss = 0.00042197\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1010, loss = 0.00030340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1011, loss = 0.00039004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1012, loss = 0.00055670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1013, loss = 0.00049515\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1014, loss = 0.00019543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1015, loss = 0.00039209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1016, loss = 0.00050882\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1017, loss = 0.00034633\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1018, loss = 0.00027579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1019, loss = 0.00019045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1020, loss = 0.00019945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1021, loss = 0.00027055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1022, loss = 0.00043224\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1023, loss = 0.00032760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1024, loss = 0.00015372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1025, loss = 0.00046715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1026, loss = 0.00030853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1027, loss = 0.00034509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1028, loss = 0.00017914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1029, loss = 0.00018837\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1030, loss = 0.00030114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1031, loss = 0.00016127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1032, loss = 0.00194202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1033, loss = 0.00080495\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1034, loss = 0.00015759\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1035, loss = 0.00025892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1036, loss = 0.00031518\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1037, loss = 0.00060911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1038, loss = 0.00063849\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1039, loss = 0.00015232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1040, loss = 0.00020407\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1041, loss = 0.00019813\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1042, loss = 0.00040641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1043, loss = 0.00043805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1044, loss = 0.00021699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1045, loss = 0.00032552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1046, loss = 0.00026065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1047, loss = 0.00019374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1048, loss = 0.00078719\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1049, loss = 0.00023460\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1050, loss = 0.00062661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "while epoch < N_EPOCHS:\n",
    "    print('epoch: ', epoch)\n",
    "    # SHUFFLING\n",
    "    random_perm = np.random.permutation(xtrain.index) #this returns a premuted series \n",
    "    mini_batch_index = 0\n",
    "    while True:\n",
    "        # MINI-BATCH\n",
    "        indices = random_perm[mini_batch_index:mini_batch_index + N_BATCH]\n",
    "        clas.partial_fit(xtrain.loc[indices], ytrain.loc[indices], classes=N_CLASSES)\n",
    "        mini_batch_index += N_BATCH\n",
    "\n",
    "        if mini_batch_index >= N_TRAIN_SAMPLES:\n",
    "            break\n",
    "\n",
    "    scores_train.append(clas.score(xtrain, ytrain)) #recording accuracy of the model after each iteration\n",
    "    pred = clas.predict(xtrain)\n",
    "    loss_train.append(log_loss(ytrain,pred))\n",
    "\n",
    "    scores_test.append(clas.score(xtest, ytest)) #recording loss of the model after each iteration\n",
    "    pred = clas.predict(xtest)\n",
    "    loss_test.append(log_loss(ytest,pred))\n",
    "    epoch += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the graph of Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores_train)\n",
    "plt.plot(scores_test)\n",
    "plt.legend([\"test\",\"train\"])\n",
    "plt.title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the graph for loss of the each iteration of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'loss')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEICAYAAABVv+9nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3xUVfr48c+Z9EISSAfSkRZ6LyqgImBnbciqa0VU1rWXXde639+66+oi6qJY1rKKDQtKkS6oSBUIVVqQENIhISRD2vn9cScYIGWSTMnced6vV16TzNy581xHnjlzynOU1hohhBDmY3F3AEIIIZxDErwQQpiUJHghhDApSfBCCGFSkuCFEMKkJMELIYRJSYIXXkMplamUusDdcQjhKpLghRDCpCTBCyGESUmCF15HKRWglJqulMq2/UxXSgXYHotSSn2jlDqqlCpSSq1SSllsjz2ilDqklDqmlNqllDrfvVciRON83R2AEG7wF2AY0A/QwFfA48BfgQeALCDaduwwQCulugHTgMFa62ylVDLg49qwhWgeacELb/R74BmtdZ7WOh94GrjB9lglEA8kaa0rtdartFGwqRoIAHoqpfy01pla671uiV4IO0mCF96oI3Cgzt8HbPcBPA/sARYppfYppR4F0FrvAe4FngLylFIfKaU6IkQbJgleeKNsIKnO34m2+9BaH9NaP6C1TgUuBe6v7WvXWn+otT7b9lwN/MO1YQvRPJLghTeaDTyulIpWSkUBTwD/A1BKXaKU6qKUUkAJRtdMtVKqm1LqPNtgrBUotz0mRJslCV54o78B64EtQAaw0XYfwFnAEqAUWA38R2u9AqP//TmgAMgBYoA/uzRqIZpJyYYfQghhTtKCF0IIk5IEL4QQJiUJXgghTEoSvBBCmJTbShVERUXp5ORkd728EEJ4pA0bNhRoraObPtKNCT45OZn169e76+WFEMIjKaUONH2UQbpohBDCpCTBCyGESUmCF0IIk5J68EIIj1JZWUlWVhZWq9XdoThVYGAgnTt3xs/Pr8XnkAQvhPAoWVlZtGvXjuTkZIyacOajtaawsJCsrCxSUlJafB7pohFCeBSr1UpkZKRpkzuAUorIyMhWf0uRBC+E8DhmTu61HHGNnpfgc7fB4ifgxDF3RyKEEG2a5yX4o7/CDy9B3g53RyKE8EJHjx7lP//5T4ueO336dMrKyhwcUcM8L8HH9DBu87a7Nw4hhFfypATvebNowhPBL0Ra8EIIt3j00UfZu3cv/fr1Y+zYscTExPDJJ59w4sQJJk6cyNNPP83x48e55ppryMrKorq6mr/+9a/k5uaSnZ3NmDFjiIqKYvny5U6P1fMSvMVitOJzt7k7EiGEmz399Ta2Z5c49Jw9O4bx5KXpDT7+3HPPsXXrVjZt2sSiRYv47LPPWLt2LVprLrvsMlauXEl+fj4dO3Zk3rx5ABQXFxMeHs6LL77I8uXLiYqKcmjMDfG8LhowEry04IUQbrZo0SIWLVpE//79GTBgADt37mT37t307t2bJUuW8Mgjj7Bq1SrCw8PdEp/nteABYnrCz+9DaT6E2lU1UwhhQo21tF1Ba81jjz3GHXfcccZjGzZsYP78+Tz22GNceOGFPPHEEy6PzzNb8LE9jds86aYRQrhWu3btOHbMmKY9btw43n77bUpLSwE4dOgQeXl5ZGdnExwczPXXX8+DDz7Ixo0bz3iuK3huCx6MbprU0e6MRAjhZSIjIxk5ciS9evViwoQJTJ48meHDhwMQGhrK//73P/bs2cNDDz2ExWLBz8+PmTNnAjBlyhQmTJhAfHy8SwZZldba6S9Sn0GDBukWb/ihNTyfBt0vhstedmxgQog2bceOHfTo0cPdYbhEfdeqlNqgtR5kz/M9s4tGKaMVnytz4YUQoiGemeDBSPD5O6Gmxt2RCCFEm+S5CT62J1SUQvFBd0cihBBtkucm+JMDrdJNI4QQ9fHcBB/d3biVBC+EEPXy3AQfGGbUpZEVrUIIUS/PTfBgq0kjLXghhOu0tJrkRRddxNGjR50QUcM8P8EX/ALVle6ORAjhJRpK8NXV1Y0+b/78+URERDgrrHp5doKPTYeaSijc4+5IhBBeom654MGDBzNmzBgmT55M7969AbjiiisYOHAg6enpzJo16+TzkpOTKSgoIDMzkx49enD77beTnp7OhRdeSHl5uVNi9bhSBYeOlvPDngIu7dORoLqbf8R4x8o2IUQdCx6FnAzHnjOuN0x4rsGH65YLXrFiBRdffDFbt24lJSUFgLfffpsOHTpQXl7O4MGDufLKK4mMjDzlHLt372b27Nm88cYbXHPNNcyZM4frr7/esdeBB7bgtxw8ysOfbWFvfilEdQXlIwOtQgi3GTJkyMnkDjBjxgz69u3LsGHDOHjwILt37z7jOSkpKfTr1w+AgQMHkpmZ6ZTYmmzBK6USgPeAOKAGmKW1fum0Y0YDXwH7bXd9rrV+xrGhGlKiQwDYV3CcXp3CIbKLDLQK4a0aaWm7SkhIyMnfV6xYwZIlS1i9ejXBwcGMHj0aq9V6xnMCAgJO/u7j4+PWLpoq4AGt9UalVDtgg1Jqsdb69Ky6Smt9ieNDPFVyZAhKwf7848YdMT3g8GZnv6wQQgCNl/wtLi6mffv2BAcHs3PnTn766ScXR3eqJhO81vowcNj2+zGl1A6gE+CWZnOgnw8dw4PYX2DUXyamJ2z/CiqOg39I408WQohWqlsuOCgoiNjY2JOPjR8/ntdee40+ffrQrVs3hg0b5sZImznIqpRKBvoDa+p5eLhSajOQDTyotT5jNw6l1BRgCkBiYmJzYz0pNTqEfQW2FnxsT0Abhcc6DWzxOYUQwl4ffvhhvfcHBASwYMGCeh+r7WePiopi69atJ+9/8MEHHR5fLbsHWZVSocAc4F6t9em73G4EkrTWfYGXgS/rO4fWepbWepDWelB0dMu32kuJCmF//nG01qdu/iGEEOIkuxK8UsoPI7l/oLX+/PTHtdYlWutS2+/zAT+llNO2DU+NCuHYiSoKSiugfTL4BkmCF0KI0zSZ4JVSCngL2KG1frGBY+Jsx6GUGmI7b6EjA60rJToUgH35pWDxgehukCv7swrhLdy1E50rOeIa7WnBjwRuAM5TSm2y/VyklJqqlJpqO+YqYKutD34GMEk78R1IjTIGU/fX9sPH9JQWvBBeIjAwkMLCQlMnea01hYWFBAYGtuo89syi+R5QTRzzCvBKqyJpho4RQfj7Wuok+B6w+UMoK4LgDq4KQwjhBp07dyYrK4v8/Hx3h+JUgYGBdO7cuVXn8LhSBQA+FkVyZDB78+vOpMEoWZB8tvsCE0I4nZ+f3ykrR0XDPK5UQa2UqJBT58KDdNMIIUQdHpvgU6ND+bWojKrqGmgXD4HhsruTEELU4bEJPiUqhMpqTdaRclAKYtKlJo0QQtThsQn+zJk0PYwuGhOPrAshRHN4boKvnQtfN8GfKIaSbDdGJYQQbYfHJvj2wX6EB/kZi53A2N0JpB9eCCFsPDbBK6VsM2lsLfjo7satJHghhAA8OMGDUVXyZIIP7mDMppGpkkIIAXh6go8K4XCxlbKKKuOOmJ5Sk0YIIWw8OsGnRBkDrafMpMnfBTXVboxKCCHaBo9O8KnR9RQdqz4BRfvcGJUQQrQNHp3gkyNtG3DXV5NGCCG8nEcn+CB/HzqGB/7Wgo/qBigZaBVCCDw8wYOx4OnkYif/YOiQIgOtQgiBCRJ8SlQI+/JLfyv+L5t/CCEEYJIEf8xaReHxCuOOmJ5QtBcqre4NTAgh3MzjE/yZM2l6gK6Bgl1ujEoIIdzP8xN8VJ0NuKFOTRrpphFCeDePT/Cd2gfh72P5baC1Qyr4+MtUSSGE1/P4BO9jUSRFBrO/di68jx9EdZXNP4QQXs/jEzzYZtLUtuBBZtIIIQRmSfDRIRwoPE51Te1UyR5QkgXWYvcGJoQQbmSKBJ8WFUpltebQkXLjjpjakgXSihdCeC9TJPgU21TJfQW1M2mkJo0QQpgjwUedVnQsPAH8Q6UFL4Twak0meKVUglJquVJqh1Jqm1LqT/Uco5RSM5RSe5RSW5RSA5wTbv0iQ/wJC/T9bbGTUkY/vMykEUJ4MXta8FXAA1rrHsAw4G6lVM/TjpkAnGX7mQLMdGiUTVBKkRId+luCB9tMmu1QW6NGCCG8TJMJXmt9WGu90fb7MWAH0Om0wy4H3tOGn4AIpVS8w6NtRKqt6NhJMT2hvAhK81wZhhBCtBnN6oNXSiUD/YE1pz3UCThY5+8szvwQQCk1RSm1Xim1Pj8/v3mRNiE1KoTsYivlFbbt+mJ6GLd5UjpYCOGd7E7wSqlQYA5wr9a65PSH63nKGX0jWutZWutBWutB0dHRzYu0CbUzaTILa3d3kpo0QgjvZleCV0r5YST3D7TWn9dzSBaQUOfvzkB268Oz3xkzaUKiICRaBlqFEF7Lnlk0CngL2KG1frGBw+YCN9pm0wwDirXWhx0YZ5NqE/z+grr98D1kLrwQwmv52nHMSOAGIEMptcl235+BRACt9WvAfOAiYA9QBtzs+FAbF+zvS3x44Gk1adJh47tQUwMWU0z5F0IIuzWZ4LXW31N/H3vdYzRwt6OCailj+766Cb4HVJbB0UyjjLAQQngRUzVrU6Pr2Z8VZKBVCOGVTJXgU6JCKbFWcaSs0rgjprtxK/3wQggvZKoEn3pyJo1toDWgHUQkykwaIYRXMleCP1lVUjb/EEIIUyX4ThFB+PmoM2vSFO6Gqgr3BSaEEG5gqgTv62MhKbKemjQ1VUaSF0IIL2KqBA/GVMlTWvCxMpNGCOGdTJfgU6NCyCws+21/1sizwOIrM2mEEF7HfAk+OoSKqhqyj9r2Z/X1h8guMpNGCOF1TJfgU6JCgdNm0nQcAHuXwdb66qQJIYQ5mTDB24qO1R1ovfBv0LE/fHYzfD9ddnkSQngF0yX4qFB/2gX6ntqCD4mEG7+C9N/Bkifhm/ugusp9QQohhAvYU03SoyilSD19Jg2AXyBc+Ra0T4Lv/w3FB+Hqd4zVrsK8tnwKodGQOtrdkQjhcqZrwUM9VSVrWSxwwVNw6Uuwdzm8PQFKXLoviXAlrWHBw7D0WXdHIoRbmDLBp0aHkl1cjrWyuv4DBt4Ev/8EjmTCG+dDToYrwxOuUpJtbLx+eBNU1POBL4TJmTLBp0SFoHWd/Vnr0+UCuGUhKAVvj4fdS1wXoHCN3K3GbU0VZK13byxCuIFpEzzA/vq6aeqK6wW3LYEOKfDhNbD+vy6ITrhMzhbbLwoO/OjWUIRwB1Mn+H2nD7TWJ6wj3LwA0s6Db+6FxU8aW/wJz5eTAe1TjA/yXyXBC+9jygQfEuBLXFhg/QOt9QloB9d9BINugR+mw5xboNLq3CCF8+VkQFxvSBxhdNFUV7o7IiFcypQJHmqLjpU2fWAtH1+4+EUY+yxs+wLeuwzKipwXoHCuE8egaB/E9YGk4cbevIc3uzsqIVzKtAk+NTrEvi6aupSCkffA1e9C9iZY9LhzghPOl7vNuK1twYP0wwuvY9oEnxIVwtGySo4cb8FGH+lXwIAbIeNTKM1zfHDC+Wqnvsb1hnax0CENfl3t3piEcDHTJvh6t+9rjqF3QHWFzKzxVDlbIKiDMYgORjfNgR9lAF14FfMm+NqqkvnN6IevK+osY678+rdkuz9PVDvAqpTxd+IIsB6F/J3ujUsIFzJtgu/cPghfizqzJk1zDL0TSnNh+5eOC0w4X3WVUf8/rvdv9yUNN25luqTwIqZN8L4+FhIjg1uX4NPOM3aE+mmmlBj2JIW7ofqEMYOmVvsUCI2DA9IPL7xHkwleKfW2UipPKbW1gcdHK6WKlVKbbD9POD7MlkmNCrV/Lnx9LBajLz57I2Stc1xgwrnqDrDWUgqSRhgDrfJhLbyEPS34d4DxTRyzSmvdz/bzTOvDcozU6BD2Fx6npqYV/6D7XgcB4UYrXniGnC3gE2CMo9SVNAJKDsHRX90TlxAu1mSC11qvBDxyxU9KlG1/1uLylp8kIBQG3ADbv4LiQ44LTjhPTgbE9AAfv1PvT7T1w8t8eOElHNUHP1wptVkptUApld7QQUqpKUqp9Uqp9fn5+Q566Yal1takaU03DcCQKYCGdW+2PijhXFpDzlaj/szpYnpCYLgMtAqv4YgEvxFI0lr3BV4GGpxyorWepbUepLUeFB0d7YCXblyKbS58qwZawdgFqttFsOEdqGzFtwHhfMdyoKzg1AHWWhYLJAyTgVbhNVqd4LXWJVrrUtvv8wE/pVRUqyNzgOjQAEIDfFuf4AGGTjU2j9jySevPJZynvgHWupKGG7NsSp3/DVIId2t1gldKxSllrCZRSg2xnbOwted1BKUUqdEh7G3pYqe6ks+G2F6w5nWZhdGW1daAj22gpzBppHErZQuEF7BnmuRsYDXQTSmVpZS6VSk1VSk11XbIVcBWpdRmYAYwSeu2kwFT6tuAuyWUMlrxedsgc1XrzyecIycD2icbfe31ie8HvkGS4IVX8G3qAK31dU08/grwisMicrCUqBDmbs7GWllNoJ9P607W+2pY8iT89BqknOuYAIVj1ZYoaIivP3QeBAd+cF1MQriJaVey1kqNDkVrOFBY1vqT+QXCwJth13wo2t/68wnHqlsDvjGJw40PAmuJa+ISwk3Mn+Br92dtzuYfjRl8G1h8YO0bjjmfcJzc7YBuvAUPxkCrroGstS4JSwh3MX2CT27O/qz2CIuHnlfAz+8bLUbRdtQOsDaV4DsPAeUj0yWF6Zk+wYcG+BIbFtD6xU51DbsTTpTAptmOO6dovZwMCGoPYZ0aPy4gFOL7yECrMD3TJ3gwBlpbXBe+Pp0HQadBsOY12UCiLTm9BnxjkkYaG3FXnXB+XEK4iVck+EFJHdh08CiZjuqmAWPKZNFe2LvUcecULVddBXnbmx5grZU43CgpfGijc+MSwo28IsHfODwJXx8Ls1btc9xJe15u1BeXKpNtQ+EeqLI23f9eK1E2ABHm5xUJPiYskKsGduaz9VnklVgdc1Jff2NGzd6lkP+LY84pWq6pEgWnC4mEqG4y0CpMzSsSPMCUc1Kpqqnh7R8yHXfSQTcbdcfXvNb859ZUw875RgEzmY/derkZ4OMPUV3tf07ScDi4xngvhDAhr0nwyVEhXNQ7ng9+OkCJtdIxJw2JMla3bp4N5Ufse461GH58BWb0h4+ug6//BC/2gPkPQ+Fex8TljRqqAd+YxBHGbKjcbc6LSwg38poED3Dn6DSOnaji/dUHHHfSYVOhsgw2vt/4cQV7YN6D8EIPWPQXCOsIV78Lty2F7hfD+rfh5YHwwTWwd5kUNGsOreHwFvu7Z2oljTBuZbqkMCmvSvDpHcMZ1TWa//6wH2ulg76Wx/WGpLONla3VVac+pjXsWQL/uwpeGQgb34Wel8GU7+CWhZB+hTHl8nez4L6tMOphY//X9yfCq0Nh3VtQ4cCZP2ZVmttwDfjGRCRAeILUpRGm5VUJHoxWfEFpBZ9uyHLcSYdNheJfjRo1YCTldW/Cq0Pgf1caKyxH/xnu2wYTX4OO/c48R7s4GGM75orXjLo38+6HF3vCor/KPqKNqR1gja1nF6emJA43BlrlG5MwoSarSZrN0JQO9E+MYNbKvVw3OAFfHwd8xnW7CMIT4YeXjEG7n983+to79oeJsyB9ojHrxh6+AdDvOug7CX79CdbMhNWvwupXoPslxiraxOH2LebxFidLFLQgwScNh4xPjCJlkWmOjUsIN/O6FrxSijtHpXGwqJx5GYcdc1KLDwy5HQ6tN+bFp50Pty6G25dD32vtT+6nBmokn2vegz9thhH3wP6V8N8JsPAxx8RtFjkZEJHUcA34xiTa+uFlI25hQl6X4AEu6BHLWTGhzFyxF4ftTTJkClz2MtybAVf/FxKGOK6VHZEAY5+G+3fAwJuMVv3uxY45txk0VQO+MdHdIKiDDLQKU/LKBG+xKKaOSmNnzjFW7HLQ3px+gTDgRghvotBVa/gHw/h/QHQP+OpuKCty3mt5ihOlxvTS5g6w1lLKmE0jLXhhQl6Z4AEu69eRjuGBzFzhYXPP/QKNWTdlRfDNfc4ZHCzNh8wfPGPgMc/OGvCNSRwOR/ZDiYO67IRoI7w2wfv5WLj93FTWZhaxPtPDWsLxfYwZN9u/hIxPHXvu8iPwzkXGz/sTIW+HY8/vaPbWgG9MktSlEebktQke4NrBCbQP9vO8VjzAyD9BwjBj8VSxg6Z8Vp2Aj2+AI5nG+bM3wsyRMP+httsdlJMBgREQ3rnl54jrC34hUpdGmI5XJ/hgf19uHpnC0p157MzxsHowFh+YOBNqquDLO1tfl15rmPtHyFwFl/8Hxj4Df/zZqLez7k14eUD9i7ncrTk14Bvi4wsJg2WgVZiOVyd4MEoJB/v78Pp3Diwl7CodUmH8343pk2tfb925VvwdtnwM5z0Ofa427guJhItfgKnfG4uI5j8Ir50Ne5e3PnZHqKk29mFt6QBrXYkjjJo05Udbfy4h2givT/ARwf5MHpLI3M3ZHCwqc3c4zTfgRug6ARY/CXk7W3aOnz+A7/4B/a+Hcx488/HYdPjD13Dt/4y6O+9fAbMnG4uD3KlwL1SVt67/vVbSCEAbC9WEMAmvT/AAt56TgkXBm47cEMRVlILLZhj7jH4xBaoqmvf8fSvg63sgdTRcMr3hrg6loMelcPdaOP9J43mvDjU+WNy1+bgjBlhrdR4EFj+pSyNMRRI8EB8exMT+nfho3UEKSj1wj87QGLh0BhzeDCv/af/z8nbAxzdC5FnGill7Su36BcI598MfN0Cvq+CH6UYVzJ8/cP3+tDktqAHfEL8go7SEDLQKE5EEb3PHqDQqqmt4x5EbgrhSj0ug3+9h1QtwcG3Txx/LhQ+uNhL27z9t/jL/sHhjkPe2ZRCRCF/dZdS3d+Um1jkZEN29ZaUg6pM0HLJ/hspyx5xPCDdrMsErpd5WSuUppbY28LhSSs1QSu1RSm1RSg1wfJjOlxYdyvj0ON5bnckxR20I4mrjn4OwzvDFHY2XGa44DrOvhbJCmPyxUQqhpToPhFsWGa/9y0L45A/N7yZqqZwMxwyw1kocATWVkLXececUwo3sacG/A4xv5PEJwFm2nymAx+5CPXVUGiXWKmav9dDSvIFhRjniov2w6PH6j6mphjm3Gd05V71tdEu0lsViVLm86F/wywL49CbnJ/ljuXA8zzH977UShwJKpksK02gywWutVwKNrXK5HHhPG34CIpRS8Y4K0JX6JkQwskskb67az4kqD92nM3kkjPijsUPUL4vOfPzbPxt16yf8E7pNcOxrD7kdJjwPu+bBZzdDtRO/CTV3k217BLWHmJ7216WprjJm8uxebEwd9YTSDsKrOKIefCfgYJ2/s2z3nVHYQyk1BaOVT2JiogNe2vHuGt2F37+5hs83HuK6IW0zxiad9zjsWQpzp8Gdq4357GCUMl7zGgyfZiRjZxg6BXQNLHwEPrvF+JbQnH1S7dWaGvCNSRoBmz40krePr/GNp/igkciL9tlu9xq3Rw8YC81qDZlidFVZfBwbkxAt5IgEX9+8unqbMlrrWcAsgEGDBrXJ5s6ItEj6dA7n9e/2cs2gBHwsHrixhm8A/O51mDUGvrnXmCGza75RR77HpTD2Wee+/rCpoKuNbwtzboMr3zKSpSPlZBiDu00MDi/IOExkaABDUjrYd96k4bDuDWOuf2muUbahuk53k18wdEgzPlh6Xm5sEtIhDXZ+Y2zKcvRX43oDQlt+bUI4iCP+1WUBdUfpOgPZDjivW9RuCHLnBxv5Zks2l/dzYvlfZ4rrbbTklzwJi58wygx0GmDsMGVxweSp4XcbLflFj9vKKsxybJK3Y4DVWlnNA59uJjYskGUPjELZU84gZbSxQris0Jh+2W2CkcBrE3m7uPrXCiQNh/bJsOBho1Db5E+MY4VwI0f8i5sLTFNKfQQMBYq11h5dd/XC9Dh6xofxzNfbGZ4aSUxYoLtDapkRf4RfvoUfZxg7Hl33kVFT3pWvr2uMDxhlgYmvO6b7ouI4FO6B3lc1etiKXXmUVVSzv+A4P+0rYnhaZNPnDomEe35uWVxDbje+VXx6M7x5gZHkY3u27FxCOIA90yRnA6uBbkqpLKXUrUqpqUqpqbZD5gP7gD3AG8BdTovWRXwsihnX9eN4RRX3fbKJmpo22ZvUNIuPMaum15Xw+8+MBVGuNvJPxsrXjE9tRdEcMHida18N+HkZOXQI8addoK/rZkZ1HQc3zzcGmN8e13bq9giv1GQLXmt9XROPa+Buh0XURnSJacdTl6bz6OcZvLZyL3eN7uLukFqmfZIx0OlO59xvtOSXPQvKBy5/pXUteTtKFFgrq1m6I5cr+nfCz6KYvfYgR45X0D7EQYuiGtOxH9y+1FhI9sFVRgmIATc4/3WFOI2sZG3EtYMTuLhPPC8s+oWNvx5xdzie7dwHYcxfYPOHMPee1pU1yMkwBlfDG16gVds9c3HveCYNSaSiuobPfz7U8tdsrvDOcMtCSD7HmM207G8yjVK4nCT4Riil+H8TexMfHsg9s3+mxFNXuLYVox6G0Y/Bpv8ZBc5amuRztxoDrI0Mmn6z5TCRIf4MTelAj/gw+iZEMHvtr47bZN0egeFGGYgBN8LK5+Hz211bykF4PUnwTQgP8uOlSf05XGzlz59nuDZBmNHoR+Hch+Hn940pnM1N8jXVRt32RrpnyiuqWbYzj3G94vD1Mf4XnzwkgT15pWw44OJvYj5+RiG4858wxiHen9h2d8cSpiMJ3g4Dk9pz/9iufLPlMJ+ud9D2eN5szJ+NuvMb34U3xsCvP9n/3KJ9Rk36RhJ83e6ZWpf06UiIvw+z1x5s8HlOoxSc84AxPz5rHbx1oftr6QuvIAneTneOSmNkl0ienLuNPXluqn9uFkoZc/SvfAuO5xuzTT671b69Ze0YYJ2X8Vv3TK2QAF8u69eJeRnZFJe7qaut91Vw41dQVmBMo9y7TPrlhVNJgreTxaJ48Zp+BPn7MO3Dn7FWemitmrZCKSPhTVsHox4xVoK+PAhWPAcVjeyslZNhbHvrdzwAABnLSURBVMwR1a3eh8srqlm649TumVqThyRirazhq00uHGw9XdIIuHUJBIQZ3TWvn2PU0q+0ui8mYVqS4JshNiyQF67uy86cYzy3oIXb44lT+YcYXTbT1kG38cbesK8Mhq1z6m/d5mRATMM14FfsyqO8sppLep9Z765353DSO4Yxe+1B946lRHWBO380pk9WVxm19P+dbsy0KfHoNYKijZEE30xjusdwy8gU3vkxk8Xbc90djnlEJMLV78DNCyC4g1Go7L8TIHvTqcc1UaKgtnumodozk4YksuNwCVuyih0YfAv4B8Ogm+Gu1Ua3TefBsPJfML2XUb8na4N74xOmIAm+BR6Z0I30jmE89NlmDhfL7j8OlTQCpqwwZp4U7IZZo+GraVCaZ9SAL81tsP+9se6ZWpf360iQn0/bqfmvlLEf7uSPjG0QB98OuxbCm+cZ/fQZnzm37LIwNUnwLRDg68PL1/WnoqqGez/aRLWnljJoqyw+MPAPcM9Go2jZ5o9gxgBY9Bfj8QYSfGPdM7XCAv24pE88czdnU3qiqsHj3CIyDSY8B/dvN+r1lxXCnFthem9jHv3xAndHKDyMJPgWSo0O5ZnLe7FmfxGvLt/j7nDMKTAcxv0f3PWTsZFJxqfG/bHp9R7+TRPdM7UmDUmkrKKarze30aKngWEw9A6YtsEoWBbd3eiff7GnUbjNWuLuCIWHkATfClcO6MQV/ToyfckvrM+UxStOE9XF2Dv2+jlw2cvGzkunKa+oZtmOPMY30j1Ta0BiBN1i27WdbpqGWCxG8bIbv4S71hhF4354CV4eCBvfb125B+EVJMG3glKKZ6/oRUKHYP700SaKy6Sv1Km6XGAs+6/Hclv3zMWNdM/UUkoxaUgCW7KK2Zbt5sFWe8V0h4kz4fZlRt35udOav0hMeB1J8K3ULtCPGZP6k1ti5ZE5W6SUgZvMyzhMVGjT3TO1JvbvhL+vhY/csbK1NToNhFsXwe/eNAaem7NITHgdSfAO0DchgofHd2PhthzeW33A3eF4ndrumXHpTXfP1IoI9ufi3vF8+fMhyis8bNGaUtDnavjj+uYtEhNeRxK8g9x2dirnd4/hb/O2S2lhF2tO90xdkwYncOxEFd9saaODrU1p7iIx4XUkwTtIbSmDuPBA7v5gI4WlUhbWVZrbPVNrSEoHUqND+Gidh3XTnK52kdhN8xtfJCa8jiR4BwoP9mPm7wdSeLyCez+W+fGu0JLumVpKKa4bnMiGA0f4JdcEBeSSR565SOzLuyH/FzcHJtxFEryD9eoUztOXpbNqdwEvLd3t7nBM72T3TJ/mdc/U+t2ATvj5qLY/ZdJepy8Sy/gEXh0M7/8Odi+WqZVeRhK8E0wanMCVAzrz8rLdrNiV5+5wTG3eFqN7ZmhKZIueHxkawIXpcXzx8yFzVQitXSR233YY87ixScoHVxnJfs0sOGGCbyyiSZLgnUApxd+u6EW32Hbc+/Emso7IzAZnKKuoYtlOY3GTj6Xh7fuaMnlIIkfLKvl2W44Do2sjQqNh1ENwb4YxtTIwHBY8ZKyKXfhnKNrv7giFE0mCd5Igfx9mXj+Q6mrN3R9s5ESViVqHbcTynfmUV1ZzUTNnz5xueGokiR2C+XCNSbpp6uPrb0ytvH0Z3LYUzroQ1r4OM/rD7Otg33cy88aEJME7UUpUCM9f3YfNWcX87Zsd7g7HdOZntK57ppbForh2cAJr9hexL7/UQdG1YZ0HwVVvGa36cx6Ag2vgvctg5kjY8C5USoVUs5AE72Tje8Vz+zkpvP/TAffuJGQyZRVVLN2Z2+rumVpXD+qMr0XxsadPmWyOsI5w/l+NfvrLXwVlga/vgVeHwsF17o5OOIAkeBd4eHx3Bie359E5GeaYjleP3BKrS1eELt+Zj7WyptXdM7Vi2gVyfo8YPtuQRUWVl8008QuE/tfD1FVww5eANkogrHweaqRr0ZNJgncBPx8Lr0weQEiAL1P/t6Ht1SFvpcPF5Vzwwnfc9YHrdiFyVPdMXZOGJFJ4vMJ7d+pSCtLGwNTvIf0Ko0Txe5dDsXzz9FR2JXil1Hil1C6l1B6l1KP1PH6TUipfKbXJ9nOb40P1bLFhgbx8XX8yC443uyiZ1pqMrGL+uXAn46evZNbKvU6MtHm01jz2eQbHTlSxfFe+S6ozOrp7pta5Z0XTKSKIj9aZeLDVHoHhcOVbcMVMOLQRZo6AHV+7OyrRAk0meKWUD/AqMAHoCVynlOpZz6Efa6372X7edHCcpjA8LZIHx3Vj3pbDvPNjZqPH1tRoNhw4wv/N2845/1zOpa98z+sr93HMWsU/Fu4iw917itrM2XiIFbvyuX9sV0IDfHntu31Of83a7pmLe3d06Hl9LIprBiWwancBM1fs5evN2azPLCLrSJn3ddsoBf0mG9027ZPh4+vh63ulmJmH8bXjmCHAHq31PgCl1EfA5cB2ZwZmVlPPTWPjgSP837wd9OkcwcCk3zavqK7RrN1fxMKth/l2Wy45JVb8fBRnd4ninvPO4oKesfgoxYXTv+OBTzfx9R/PJsDXx23Xklti5ZmvtzE4uT3TxnTheEUVb6zcxwNju5IcFeK0152XkU1UaECza8/Y49rBCbz/0wH+sXDnKfcrBZEhAcSHBxIXHkhcmHEbb/s9OSqEjhFBDo/H7SLT4NbFsOxZ+HEG/LraaN3H9XJ3ZMIOqqmuAqXUVcB4rfVttr9vAIZqrafVOeYm4O9APvALcJ/W+ozpCEqpKcAUgMTExIEHDnhnad3iskoueWUVVdWaL+8eya6cYyzYmsPi7TkUlFYQ4GthVNdoJvSO47zusYQH+Z3y/BW78rjpv+uYOiqNRyd0d8s1aK25/b0NrNqdz8J7zyUlKoS8Eitn/3M5Vw3szP+bWP++qa1VVlHFgGcXc9XAzvztCue8htaaEmsVOcVWckqs5BSXc7jYSm6JlcPF1pP3Hz1tg5ce8WFc1CuOCb3j6BLTzimxudXe5fDFHVB+FMY+Y2wrqBzXRSbso5TaoLUeZM+x9rTg63sHT/9U+BqYrbU+oZSaCrwLnHfGk7SeBcwCGDRokNeuqqgtSva7mT8y/O9LqdEQ4u/DmO4xTOgVz+hu0YQENPzWjO4Ww6TBCcxauZcL02MZkHjmFnbONndzNkt25PKXi3qQYmutx4QFctXAzny2Pot7zz+LmLBAh7/usp15TumeqUspRXiQH+FBfnSLazhRl1dU2z4ArGzLLmbh1hxeWPwLLyz+hS4xoUzoFcf4XnH0jA9DmSERpo2BO3+Er+6GhY/A3mVwxX8gJMrdkYkG2NOCHw48pbUeZ/v7MQCt9d8bON4HKNJahzd23kGDBun169e3KGizWJBxmBW78jm/Rwzndo0m0M/+7pZj1krGT19FgK+FefecQ5C/67pq8o+d4MJ/f0dSZAhz7hxxykBnZsFxznthBVPOdc63i7s+2MDa/UdY8+fzHTrA6ii5JVa+3ZbDgowc1uwvpEZDUmQw43vFMaFXPH07h3t+stca1r4Bix6HoAiY+BqkndGeE07SnBa8PQneF6Pb5XzgELAOmKy13lbnmHit9WHb7xOBR7TWwxo7ryT41vtxTwGT31zDLSNTeOLS+sa9neOuDzawZHse8+45m7Niz2zhTvtwI9/tyueHx84jLNCvnjO0TG33zNUDE3j2irbfB1xYeoLF23NZsDWHH/YUUFWj6RgeyDhbsh+Y1L5NfkjZLWcrzLkV8neCf6i7o6lfaKwxjtAhzXabatyGJxiVNz2QQ7totNZVSqlpwLeAD/C21nqbUuoZYL3Wei5wj1LqMqAKKAJuanH0wm4jukRx4/Ak/vvjfsalxzI01XFzwhsyP+Mw8zNyeGhct3qTO8DUUWl8s+Uw768+wN1jujjstZfuyHPo4iZniwwNYNKQRCYNSaS4rJIlO4xk/8GaX/nvD5lc3DueV38/wN1htlxcL7h9uVHT5niBu6M5k66BkkNQuA8yv4fKOjOAfPyN2UGnJ/4OaRDWCSzmWCLUZAveWaQF7xjHT1Qx4aVVaDQL/3Ruo333rVV0vIKxL35Hx4ggvrhrRKMbbNz49lq2Zxfz/SPnNavrqSGlJ6qY8NJKLEqx7IHRHt3yLT1RxYylu5m1ch8fTRnGMBd8MHs9reFYDhTthcK9dW73GT9V1t+O9Q2E9ilnJv7INGgX7/aBZUcPsoo2LCTAl39d3ZdrZ63m7wt2OG1mCcBTc7dRYq3kg6uHNrl70l2j05g06yc+3ZDFDcOSWv3af/tmO4eOlPPJHcM9OrkDhAb4cv/YrnyzOZu/L9jJl3eN8Px++bZOKQiLN36Szz71sZoaOJZ9ZuIv2A27F0F1xW/H+gbZkn7qb0k/rjfE9W2TrX5J8CYwJKUDt4xM4a3v9zM+PZ6zz3L8rIZF23KYuzmb+y7oSve4sCaPH5rSgf6JEcxauZfrBic0ezu9upZsz+WjdQe5c3Qag5IdP/fdHQL9fLj/wm48+Olm5mfktHhHKuEAFguEdzZ+Uked+lhNta2bpzb57zNu83bCroVQY5sqGxINXcbCWRcYA85Brp/ZVh/pojEJa2U1F81YhbWimoX3nevQwc2jZRWM/fdKokIDmDttJH52JutF23KY8v4GXprUj8v7dWrRaxeWnmDc9JVEtwvkq7tH4u/b9lpJLVVdo7l4xiqsldUsum+Uqa7NK1RXQfFByFpntPT3LIHyI6B8IGEInDXWqLsf28uh3TrN6aKR/6NMItDPhxeu7ktOiZW/fePYRcbPfrODI8creP6qPnYnd4ALesTSJSaUmSv2Nqv2Tq3aOjcl5VVMv7af6RKgj0XxyITuZBaWmWdPWG/i4wsdUqDPNXDlm/DQXmPV7zn3GwO6S5+B186GF3vAV9Ng+1ywlrg0RHP9i/Fy/RPbc8eoND5Zn8XynY7ZC3b5rjzmbMziztFp9OrU6NKGM1gsiqmj0tiZc4wVu/Kb/dqfbchi0fZcHhrXrdEFR55sdNdohqdGMmPpbo5ZK5t+gmi7LLaW+3mPwx0r4YFdRp39hKGw/Sv45Ab4Zwq8cwls+8I1IbnkVYTL3HvBWXSLbcejn2+huKx1CaPEWsljczLoGhvKtPNaNt3x8n4d6RgeyMwVzauAebCojKe/3s7QlA7cenZKi17bEyileOyi7hQer+CNlc4v1CZcqF2cUWf/mnfh4X1w03wYPg3KiqA4yyUhSII3mQBfH164pi+FpRU89fW2pp/QiP83bwd5x6w8f1XfFhc18/OxcPu5qazNLGJ9ZpFdz6mu0Tzw6WYAXrimLxYPnzXTlD6dI7ikTzxvrNpPXom16ScIz+PjB8kjYezTcNePMOxul7ysJHgT6tUpnLvHdOGLnw/x7bacFp1j1e58Plp3kNvPTaVvQkSr4rl2cALtg/3sbsW/9f0+1u4v4qnL0uncPrhVr+0pHhrXjaqaGqYv3e3uUIQruGhKpSR4k5p2XhfSO4bxly8yKDpe0fQT6jhmreTRORmkRodw3wVdWx1LsL8vN41IYenOPHbmND7ItDOnhH99+wvj0mO5ckDLZt54oqTIEH4/NImP1x1krzds/C1cQqZJmtjOnBIuffl7zu8ey5RRqRwtq+BoWSVHyiopLqvgSFklR8sr69xfQXFZJcdOVKEUfDZ1OAOTHDPv/GhZBSOeW8a49Dj+fW2/eo85UVXN5a/8QEFpBd/eew6RoQEOeW1PUVh6glHPr2Bkl0hev8GuWXDCC8lKVgFA97gw7r2gK89/u4uFp3XVKAXhQX60D/YnPMiPqFB/usSEEhHsR0SQPwOSIhyW3AEigv2ZPCSR//6Yyf1ju5LQ4cyul38v3s3OnGO89YdBXpfcwahdc8e5qbyw+Bc2HChy6H9/4Z0kwZvcnaPS6N0pnGqtibAl9IhgP8IC/Vw+eHnrOSm8uzqTN1ft4+nLT60GuS6ziNdX7uW6IQmc3yPWpXG1Jbeek8L7Px3g7/N38unU4VLCQLSK9MGbnMWiOLdrNGO6xdA/sT3JUSFEBPu7ZWZKfHgQE/t34qN1BykoPXHy/tITVdz/ySYS2gfz+MWuK3vcFgX7+3LvBV1Zf+AIi7fnujsc4eEkwQuXumNUGhXVNbzzQ+bJ+5792igk9uI1fZ1aDdNTXDOoM2nRIfxj4U6qqr1ss2/hUJLghUulRYcyPj2O91ZncsxayeLtuXy8/iBTR5mnkFhr+fpYeGR8d/bmH+fTDa5ZECPMSRK8cLmpo9IosVbxyrI9PPb5FnrGG4PB4jdje8YyKKk9/178C2UVVe4OR3goSfDC5fomRBhTAVfuo6S8in+bsJBYa9WWMMg7doK3Vu13dzjCQ8m/KuEWd4/pglLw8HjzFhJrrYFJHRiXHsvrK/dRWGdQWgh7SYIXbjEiLYp1f7mA285JdXcobdrD47tTXlnNy8v2uDsU4YFkyoJwmygvXMzUXGnRoVw7OIEP1hzg5pHJJEWGNHp8VXUN2Uet7C88TmbBcYrLK0nvGEa/hAivXDzm7STBC9HG3Xv+WXyx8RDPf7uLVyYPoKZGk11czv4CI4nvLygj05bQDx4po7K6/vIjiR2C6ZcQQf/ECPolRNCzY1iLq4QKzyAJXog2LiYskNvPSWHGsj3szPmOX4vKqKj6bX58oJ+F5MgQusW148L0OFKigkmODCElKoTQQF+2Hirh51+PsOngUdbuL2Lu5mwA/H0s9OwYdjLhD0hsT+f2QbJ61kSk2JgQHqD0RBX3zP4ZX4siJSqE5KiQk0k8NiygWUn5cHE5m349yqaDR/n516NsOXQUa6XxgREZ4s+wtEjGp8cxpnsMobLwrM1pTrExSfBCeLmq6hp25hw7mfC/+yWfgtIT+PtaOPesaCb0iuOCHrGEBztuI3fRcpLghRAtVl2j2XDgCAu2Hmbh1hwOF1vxtShGdoliQq84xvaMlQFbN5IEL4RwCK01m7OKWZBxmAVbc/i1qAyLgqEpkUzoHce49DhiwwLdHaZXcXiCV0qNB14CfIA3tdbPnfZ4APAeMBAoBK7VWmc2dk5J8EJ4Fq012w+XsHBrDgu25rAnrxSloH9CBPHhQU5//UA/H+LDA4kLDyQ+PJDYMOO2Q4i/Vw0MOzTBK6V8gF+AsUAWsA64Tmu9vc4xdwF9tNZTlVKTgIla62sbO68keCE82568YyzIyGHpzjxKTzi/Xk6ptYq8Y1ZqTktZ/r4W4sICjR9b8o8LN/5uF+hHoJ+FQD+fOre2H18Lvj6et9bT0Ql+OPCU1nqc7e/HALTWf69zzLe2Y1YrpXyBHCBaN3JySfBCiOaqrtEUlJ7gcLGVnOJy47bESk6xlcPFVnJLjNu600gb4+ejCPT1IcD2ARDga8Higm8D1w5OaPEqbkdv2dcJOFjn7yxgaEPHaK2rlFLFQCRQcFpgU4ApAImJifbEJ4QQJ/lYFLFhRvcMCRH1HqO15khZJTnFVo5XVFFeUY21shprVQ3WimqsVba/K2sor/zt9xOVxmOu4KpV3PYk+Po+zk5vmdtzDFrrWcAsMFrwdry2EEI0i1KKDiH+dAjxd3cobmdPB1QWkFDn785AdkPH2LpowoEiRwQohBCiZexJ8OuAs5RSKUopf2ASMPe0Y+YCf7D9fhWwrLH+dyGEEM7XZBeNrU99GvAtxjTJt7XW25RSzwDrtdZzgbeA95VSezBa7pOcGbQQQoim2VVoQms9H5h/2n1P1PndClzt2NCEEEK0hudNAhVCCGEXSfBCCGFSkuCFEMKkJMELIYRJua2apFIqHzjQwqdHcdoqWS/jzdfvzdcO3n39cu2GJK11tD1PcluCbw2l1Hp7azGYkTdfvzdfO3j39cu1N//apYtGCCFMShK8EEKYlKcm+FnuDsDNvPn6vfnawbuvX669mTyyD14IIUTTPLUFL4QQogmS4IUQwqQ8LsErpcYrpXYppfYopR51dzyupJTKVEplKKU2KaVMv9+hUuptpVSeUmprnfs6KKUWK6V2227buzNGZ2ng2p9SSh2yvf+blFIXuTNGZ1FKJSilliuldiiltiml/mS731ve+4auv9nvv0f1wduzAbiZKaUygUFaa69Y7KGUOhcoBd7TWvey3fdPoEhr/ZztA7691voRd8bpDA1c+1NAqdb6X+6MzdmUUvFAvNZ6o1KqHbABuAK4Ce947xu6/mto5vvvaS34IcAerfU+rXUF8BFwuZtjEk6itV7JmTuDXQ68a/v9XYz/8U2ngWv3Clrrw1rrjbbfjwE7MPZ99pb3vqHrbzZPS/D1bQDeogv3UBpYpJTaYNvA3BvFaq0Pg/EPAYhxczyuNk0ptcXWhWPKLoq6lFLJQH9gDV743p92/dDM99/TErxdm3ub2Eit9QBgAnC37Wu88B4zgTSgH3AYeMG94TiXUioUmAPcq7UucXc8rlbP9Tf7/fe0BG/PBuCmpbXOtt3mAV9gdFl5m1xbH2VtX2Wem+NxGa11rta6WmtdA7yBid9/pZQfRnL7QGv9ue1ur3nv67v+lrz/npbg7dkA3JSUUiG2AReUUiHAhcDWxp9lSnU3eP8D8JUbY3Gp2uRmMxGTvv9KKYWxz/MOrfWLdR7yive+oetvyfvvUbNoAGxTg6bz2wbg/+fmkFxCKZWK0WoHYy/dD81+7Uqp2cBojFKpucCTwJfAJ0Ai8CtwtdbadIORDVz7aIyv5xrIBO6o7ZM2E6XU2cAqIAOosd39Z4x+aG947xu6/uto5vvvcQleCCGEfTyti0YIIYSdJMELIYRJSYIXQgiTkgQvhBAmJQleCCFMShK8EEKYlCR4IYQwqf8PG6loH5BC+IcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train)\n",
    "plt.plot(loss_test)\n",
    "plt.legend([\"test\",\"train\"])\n",
    "plt.title(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clas.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is 99.01515151515152\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score is \"+str(accuracy_score(ytest,pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score is 96.26168224299066\n"
     ]
    }
   ],
   "source": [
    "print(\"precision score is \"+str(precision_score(ytest,pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is 96.94117647058823\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score is \"+str(f1_score(ytest,pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score is 97.6303317535545\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall score is \"+str(recall_score(ytest,pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1101,    8],\n",
       "       [   5,  206]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytest,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
